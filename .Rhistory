trControl = cv.ctrl)
rf.grid <- data.frame(.mtry = c(3:10))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp + Parch
+ Cabin + Embarked + Title + Child
+ FamilySize,
data = train.batch,
method = "rf",
metric = "ROC",
tuneGrid = rf.grid,
trControl = cv.ctrl)
rf.tune
rf.pred <- predict(rf.tune, test.batch)
confusionMatrix(rf.pred, test.batch$Survived)
Survived <- predict(rf.tune, newdata = test_m)
# reformat predictions to 0 or 1 and link to PassengerId in a data frame
Survived <- revalue(Survived, c("Survived" = 1, "Perished" = 0))
predictions <- as.data.frame(Survived)
predictions$PassengerId <- test_m$PassengerId
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
file="rf_tune_model3.csv", row.names=FALSE, quote=FALSE)
rf.grid <- data.frame(.mtry = c(3:10))
set.seed(35)
rf.tune <- train(Survived ~ .,
data = train.batch,
method = "rf",
metric = "ROC",
tuneGrid = rf.grid,
trControl = cv.ctrl)
# Lorenzo Luciano
# May 20, 2014
# Titanic Competetion on Kaggle
#set working directory
#setwd("C:/Users/llucia1/Google Drive/Kaggle/Titanic") # at work
setwd("C:/Users/lorenzol/Google Drive/Kaggle/Titanic") # at home
#Load Data
source("titanic_data_prep.R")
#Fitting a Model
#######################################################
## split training data into train batch and test batch
#caret: Classification and Regression Training
#install.packages("caret")
library(caret)
set.seed(23)
training.rows <- createDataPartition(train_m$Survived,
p = 0.8, list = FALSE)
train.batch <- train_m[training.rows, ]
test.batch <- train_m[-training.rows, ]
#Modeling
#########################################################
#install.packages("pROC")
library(pROC)
## Define control function to handle optional arguments for train function
## Models to be assessed based on largest absolute area under ROC curve
cv.ctrl <- trainControl(method = "repeatedcv", repeats = 3,
summaryFunction = twoClassSummary,
classProbs = TRUE)
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
#rf.tune->You improved on your best score by 0.00478 to 0.80383
rf.grid <- data.frame(.mtry = c(3:10))
set.seed(35)
rf.tune <- train(Survived ~ .,
data = train.batch,
method = "rf",
metric = "ROC",
tuneGrid = rf.grid,
trControl = cv.ctrl)
rf.grid <- data.frame(.mtry = c(3:10))
set.seed(35)
rf.tune <- train(Survived ~ .,
data = train.batch,
method = "rf",
metric = "ROC",
#tuneGrid = rf.grid,
trControl = cv.ctrl)
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
#rf.tune->You improved on your best score by 0.00478 to 0.80383
rf.grid <- data.frame(.mtry = c(3:10))
set.seed(35)
rf.tune <- train(Survived ~ Pclass, Sex, Age, SibSp,Parch, Ticket
Fare, Cabin, Embarked, Title, Child, Fare2, FamilySize,
FamilyID,
data = train.batch,
method = "rf",
metric = "ROC",
#tuneGrid = rf.grid,
trControl = cv.ctrl)
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
#rf.tune->You improved on your best score by 0.00478 to 0.80383
rf.grid <- data.frame(.mtry = c(3:10))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp + Parch + Ticket
+ Fare + Cabin + Embarked + Title + Child + Fare2 + FamilySize +
FamilyID,
data = train.batch,
method = "rf",
metric = "ROC",
#tuneGrid = rf.grid,
trControl = cv.ctrl)
# Random Forest
#################################################
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
#rf.tune->You improved on your best score by 0.00478 to 0.80383
rf.grid <- data.frame(.mtry = c(3:10))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp + Parch
+ Cabin + Embarked + Title + Child
+ FamilySize,
data = train.batch,
method = "rf",
metric = "ROC",
#tuneGrid = rf.grid,
trControl = cv.ctrl)
# Lorenzo Luciano
# May 20, 2014
# Titanic Competetion on Kaggle
#set working directory
#setwd("C:/Users/llucia1/Google Drive/Kaggle/Titanic") # at work
setwd("C:/Users/lorenzol/Google Drive/Kaggle/Titanic") # at home
#Load Data
source("titanic_data_prep.R")
#Fitting a Model
#######################################################
## split training data into train batch and test batch
#caret: Classification and Regression Training
#install.packages("caret")
library(caret)
set.seed(23)
training.rows <- createDataPartition(train_m$Survived,
p = 0.8, list = FALSE)
train.batch <- train_m[training.rows, ]
test.batch <- train_m[-training.rows, ]
#Modeling
#########################################################
#install.packages("pROC")
library(pROC)
## Define control function to handle optional arguments for train function
## Models to be assessed based on largest absolute area under ROC curve
cv.ctrl <- trainControl(method = "repeatedcv", repeats = 3,
summaryFunction = twoClassSummary,
classProbs = TRUE)
# Random Forest
#################################################
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
#rf.tune->You improved on your best score by 0.00478 to 0.80383
rf.grid <- data.frame(.mtry = c(3:10))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp + Parch
+ Cabin + Embarked + Title + Child
+ FamilySize,
data = train.batch,
method = "rf",
metric = "ROC",
#tuneGrid = rf.grid,
trControl = cv.ctrl)
# Random Forest
#################################################
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
#rf.tune->You improved on your best score by 0.00478 to 0.80383
rf.grid <- data.frame(.mtry = c(3:10))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp + Parch
+ Cabin + Fare + Embarked + Title + Child
+ Fare2 + FamilySize + FamilyID,
data = train.batch,
method = "rf",
metric = "ROC",
#tuneGrid = rf.grid,
trControl = cv.ctrl)
rf.tune
rf.pred <- predict(rf.tune, test.batch)
confusionMatrix(rf.pred, test.batch$Survived)
Survived <- predict(rf.tune, newdata = test_m)
# reformat predictions to 0 or 1 and link to PassengerId in a data frame
Survived <- revalue(Survived, c("Survived" = 1, "Perished" = 0))
predictions <- as.data.frame(Survived)
predictions$PassengerId <- test_m$PassengerId
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
file="rf_tune_model3.csv", row.names=FALSE, quote=FALSE)
# Random Forest
#################################################
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
#rf.tune->You improved on your best score by 0.00478 to 0.80383
rf.grid <- data.frame(.mtry = c(3:10))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp + Parch
+ Cabin + Embarked + Title + Child
+ Fare2 + FamilySize + FamilyID,
data = train.batch,
method = "rf",
metric = "ROC",
#tuneGrid = rf.grid,
trControl = cv.ctrl)
rf.tune
Survived <- predict(rf.tune, newdata = test_m)
Survived <- revalue(Survived, c("Survived" = 1, "Perished" = 0))
predictions <- as.data.frame(Survived)
predictions$PassengerId <- test_m$PassengerId
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
file="rf_tune_model3.csv", row.names=FALSE, quote=FALSE)
# Random Forest
#################################################
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
#rf.tune->You improved on your best score by 0.00478 to 0.80383
rf.grid <- data.frame(.mtry = c(3:10))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp + Parch
+ Cabin + Embarked + Title + Child
+ Fare + Fare2 + FamilySize + FamilyID,
data = train.batch,
method = "rf",
metric = "ROC",
#tuneGrid = rf.grid,
trControl = cv.ctrl)
rf.tune
Survived <- predict(rf.tune, newdata = test_m)
Survived <- revalue(Survived, c("Survived" = 1, "Perished" = 0))
predictions <- as.data.frame(Survived)
predictions$PassengerId <- test_m$PassengerId
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
file="rf_tune_model3.csv", row.names=FALSE, quote=FALSE)
# Lorenzo Luciano
# May 20, 2014
# Titanic Competition on Kaggle using Random Forests
#set working directory
#setwd("C:/Users/llucia1/Google Drive/Kaggle/Titanic") # at work
setwd("C:/Users/lorenzol/Google Drive/Kaggle/Titanic") # at home
source("titanic_data_prep.R")
#Fitting a Model
#######################################################
## split training data into train batch and test batch
#caret: Classification and Regression Training
#install.packages("caret")
library(caret)
set.seed(23)
training.rows <- createDataPartition(train_m$Survived,
p = 0.8, list = FALSE)
train.batch <- train_m[training.rows, ]
test.batch <- train_m[-training.rows, ]
#Modeling
#########################################################
#install.packages("pROC")
library(pROC)
## Define control function to handle optional arguments for train function
## Models to be assessed based on largest absolute area under ROC curve
cv.ctrl <- trainControl(method = "repeatedcv", repeats = 3,
summaryFunction = twoClassSummary,
classProbs = TRUE)
# Random Forest
#################################################
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
rf.grid <- data.frame(.mtry = c(2, 3))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp + Parch
+ Cabin + Embarked + Title + Child
+ FamilySize,
data = train.batch,
method = "rf",
metric = "ROC",
tuneGrid = rf.grid,
trControl = cv.ctrl)
rf.tune
Survived <- predict(rf.tune, newdata = test_m)
# reformat predictions to 0 or 1 and link to PassengerId in a data frame
Survived <- revalue(Survived, c("Survived" = 1, "Perished" = 0))
predictions <- as.data.frame(Survived)
predictions$PassengerId <- test_m$PassengerId
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
file="rf_tune_model1.csv", row.names=FALSE, quote=FALSE)
# Lorenzo Luciano
# May 20, 2014
# Titanic Competition on Kaggle using Random Forests
#set working directory
#setwd("C:/Users/llucia1/Google Drive/Kaggle/Titanic") # at work
setwd("C:/Users/lorenzol/Google Drive/Kaggle/Titanic") # at home
source("titanic_data_combine.R")
#Fitting a Model
#######################################################
## split training data into train batch and test batch
#caret: Classification and Regression Training
#install.packages("caret")
library(caret)
set.seed(23)
training.rows <- createDataPartition(train_m$Survived,
p = 0.8, list = FALSE)
train.batch <- train_m[training.rows, ]
test.batch <- train_m[-training.rows, ]
#Modeling
#########################################################
#install.packages("pROC")
library(pROC)
## Define control function to handle optional arguments for train function
## Models to be assessed based on largest absolute area under ROC curve
cv.ctrl <- trainControl(method = "repeatedcv", repeats = 3,
summaryFunction = twoClassSummary,
classProbs = TRUE)
# Random Forest
#################################################
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
rf.grid <- data.frame(.mtry = c(2, 3))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp
+ Parch + Cabin + Embarked + Title + Child
+ FamilySize,
data = train.batch,
method = "rf",
metric = "ROC",
tuneGrid = rf.grid,
trControl = cv.ctrl)
rf.tune
Survived <- predict(rf.tune, newdata = test_m)
# reformat predictions to 0 or 1 and link to PassengerId in a data frame
Survived <- revalue(Survived, c("Survived" = 1, "Perished" = 0))
predictions <- as.data.frame(Survived)
predictions$PassengerId <- test_m$PassengerId
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
file="rf_tune_model2.csv", row.names=FALSE, quote=FALSE)
# Lorenzo Luciano
# May 20, 2014
# Titanic Competition on Kaggle using Random Forests
#set working directory
#setwd("C:/Users/llucia1/Google Drive/Kaggle/Titanic") # at work
setwd("C:/Users/lorenzol/Google Drive/Kaggle/Titanic") # at home
source("titanic_data_combine.R")
#Fitting a Model
#######################################################
## split training data into train batch and test batch
#caret: Classification and Regression Training
#install.packages("caret")
library(caret)
set.seed(23)
training.rows <- createDataPartition(train_m$Survived,
p = 0.8, list = FALSE)
train.batch <- train_m[training.rows, ]
test.batch <- train_m[-training.rows, ]
#Modeling
#########################################################
#install.packages("pROC")
library(pROC)
## Define control function to handle optional arguments for train function
## Models to be assessed based on largest absolute area under ROC curve
cv.ctrl <- trainControl(method = "repeatedcv", repeats = 3,
summaryFunction = twoClassSummary,
classProbs = TRUE)
# Random Forest
#################################################
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
rf.grid <- data.frame(.mtry = c(2, 3))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp
+ Parch + Cabin + Embarked + Title + Child
+ FamilySize,
data = train.batch,
method = "rf",
metric = "ROC",
#tuneGrid = rf.grid,
trControl = cv.ctrl)
rf.tune
Survived <- predict(rf.tune, newdata = test_m)
# reformat predictions to 0 or 1 and link to PassengerId in a data frame
Survived <- revalue(Survived, c("Survived" = 1, "Perished" = 0))
predictions <- as.data.frame(Survived)
predictions$PassengerId <- test_m$PassengerId
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
file="rf_tune_model2.csv", row.names=FALSE, quote=FALSE)
# Lorenzo Luciano
# May 20, 2014
# Titanic Competition on Kaggle using Random Forests
#set working directory
#setwd("C:/Users/llucia1/Google Drive/Kaggle/Titanic") # at work
setwd("C:/Users/lorenzol/Google Drive/Kaggle/Titanic") # at home
source("titanic_data_combine.R")
#Fitting a Model
#######################################################
## split training data into train batch and test batch
#caret: Classification and Regression Training
#install.packages("caret")
library(caret)
set.seed(23)
training.rows <- createDataPartition(train_m$Survived,
p = 0.8, list = FALSE)
train.batch <- train_m[training.rows, ]
test.batch <- train_m[-training.rows, ]
#Modeling
#########################################################
#install.packages("pROC")
library(pROC)
## Define control function to handle optional arguments for train function
## Models to be assessed based on largest absolute area under ROC curve
cv.ctrl <- trainControl(method = "repeatedcv", repeats = 3,
summaryFunction = twoClassSummary,
classProbs = TRUE)
# Random Forest
#################################################
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
rf.grid <- data.frame(.mtry = c(2, 3))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp
+ Parch + Cabin + Embarked + Title + Child
+ FamilySize,
data = train.batch,
method = "rf",
metric = "ROC",
maximize = FALSE,
#tuneGrid = rf.grid,
trControl = cv.ctrl)
rf.tune
Survived <- predict(rf.tune, newdata = test_m)
# reformat predictions to 0 or 1 and link to PassengerId in a data frame
Survived <- revalue(Survived, c("Survived" = 1, "Perished" = 0))
predictions$PassengerId <- test_m$PassengerId
predictions <- as.data.frame(Survived)
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
file="rf_tune_model2.csv", row.names=FALSE, quote=FALSE)
predictions$PassengerId <- test_m$PassengerId
predictions$PassengerId <- test_m$PassengerId
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
file="rf_tune_model2.csv", row.names=FALSE, quote=FALSE)
# Lorenzo Luciano
# May 20, 2014
# Titanic Competition on Kaggle using Random Forests
#set working directory
#setwd("C:/Users/llucia1/Google Drive/Kaggle/Titanic") # at work
setwd("C:/Users/lorenzol/Google Drive/Kaggle/Titanic") # at home
source("titanic_data_combine.R")
#Fitting a Model
#######################################################
## split training data into train batch and test batch
#caret: Classification and Regression Training
#install.packages("caret")
library(caret)
set.seed(23)
training.rows <- createDataPartition(train_m$Survived,
p = 0.8, list = FALSE)
train.batch <- train_m[training.rows, ]
test.batch <- train_m[-training.rows, ]
#Modeling
#########################################################
#install.packages("pROC")
library(pROC)
## Define control function to handle optional arguments for train function
## Models to be assessed based on largest absolute area under ROC curve
cv.ctrl <- trainControl(method = "repeatedcv", repeats = 3,
summaryFunction = twoClassSummary,
classProbs = TRUE)
# Random Forest
#################################################
# Strobl et al suggested setting mtry at the square root of the number of variables.
# In this case, that would be mtry = 2,
rf.grid <- data.frame(.mtry = c(2, 3))
set.seed(35)
rf.tune <- train(Survived ~ Pclass + Sex + Age + SibSp
+ Parch + Cabin + Embarked + Title + Child
+ FamilySize,
data = train.batch,
method = "rf",
metric = "ROC",
maximize = TRUE,
#tuneGrid = rf.grid,
trControl = cv.ctrl)
rf.tune
Survived <- predict(rf.tune, newdata = test_m)
# reformat predictions to 0 or 1 and link to PassengerId in a data frame
predictions <- as.data.frame(Survived)
Survived <- revalue(Survived, c("Survived" = 1, "Perished" = 0))
predictions$PassengerId <- test_m$PassengerId
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
file="rf_tune_model2.csv", row.names=FALSE, quote=FALSE)
Survived <- predict(rf.tune, newdata = test_m)
# reformat predictions to 0 or 1 and link to PassengerId in a data frame
Survived <- revalue(Survived, c("Survived" = 1, "Perished" = 0))
predictions <- as.data.frame(Survived)
predictions$PassengerId <- test_m$PassengerId
# write predictions to csv file for submission to Kaggle
write.csv(predictions[,c("PassengerId", "Survived")],
file="rf_tune_model2.csv", row.names=FALSE, quote=FALSE)
install.packages(c("BradleyTerry2", "car", "caret", "caTools", "e1071", "gplots", "gtools", "KernSmooth", "labeling", "lme4", "pROC", "randomForest", "rattle", "RcppEigen", "RCurl"))
x*y
x*y
x <- 2
y <- 3
x*y
z <- rnorm(100,100,5)
z
hist(z)
line(z)
summary(z)
sd(z)
